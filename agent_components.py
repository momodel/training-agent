from typing import Dict, Any, List
import os
import json
import pandas as pd
from openai import OpenAI
from utils import RetryHandler, LLMError, ValidationError, validate_llm_response
import logging
from prompts import (
    get_analysis_prompt,
    get_theory_prompt,
    get_code_prompt,
    get_exercises_prompt,
    get_content_organization_prompt
)

class DatasetAnalyzer:
    """æ•°æ®é›†åˆ†æç»„ä»¶"""
    def __init__(self, llm_client: OpenAI, *, config=None):
        self.llm = llm_client
        self.retry_handler = RetryHandler()
        self.logger = logging.getLogger(__name__)
        self.max_preview_rows = config.preview.max_preview_rows if config else 5
        self.max_preview_size = config.preview.max_preview_size if config else 2000
        self.model = config.llm.model if config else "gpt-4o"
    
    def _get_dataset_preview(self, dataset_path: str) -> str:
        """è·å–æ•°æ®é›†é¢„è§ˆä¿¡æ¯"""
        file_type = self._get_file_type(dataset_path)
        preview_info = []
        
        try:
            if file_type == 'csv':
                df = pd.read_csv(dataset_path, nrows=self.max_preview_rows)
                preview_info.extend([
                    "æ•°æ®é›†åŸºæœ¬ä¿¡æ¯ï¼š",
                    f"æ€»è¡Œæ•°ï¼š{len(pd.read_csv(dataset_path, usecols=[0]))}", # åªè¯»å–ç¬¬ä¸€åˆ—æ¥è·å–æ€»è¡Œæ•°
                    f"åˆ—æ•°ï¼š{len(df.columns)}",
                    f"åˆ—åï¼š{', '.join(df.columns)}",
                    "\næ•°æ®ç±»å‹ï¼š",
                    df.dtypes.to_string(),
                    "\nå‰å‡ è¡Œæ•°æ®ï¼š",
                    df.head().to_string()
                ])
            
            elif file_type == 'excel':
                df = pd.read_excel(dataset_path, nrows=self.max_preview_rows)
                preview_info.extend([
                    "æ•°æ®é›†åŸºæœ¬ä¿¡æ¯ï¼š",
                    f"æ€»è¡Œæ•°ï¼š{len(pd.read_excel(dataset_path, usecols=[0]))}",
                    f"åˆ—æ•°ï¼š{len(df.columns)}",
                    f"åˆ—åï¼š{', '.join(df.columns)}",
                    "\næ•°æ®ç±»å‹ï¼š",
                    df.dtypes.to_string(),
                    "\nå‰å‡ è¡Œæ•°æ®ï¼š",
                    df.head().to_string()
                ])
            
            else:
                raise ValueError(f"ä¸æ”¯æŒçš„æ–‡ä»¶ç±»å‹ï¼š{file_type}")
            
            # æ·»åŠ ç¼ºå¤±å€¼ä¿¡æ¯
            missing_info = df.isnull().sum()
            if missing_info.any():
                preview_info.extend([
                    "\nç¼ºå¤±å€¼ä¿¡æ¯ï¼š",
                    missing_info.to_string()
                ])
            
            # æ·»åŠ æ•°å€¼åˆ—çš„åŸºæœ¬ç»Ÿè®¡ä¿¡æ¯
            numeric_cols = df.select_dtypes(include=['int64', 'float64']).columns
            if not numeric_cols.empty:
                preview_info.extend([
                    "\næ•°å€¼åˆ—ç»Ÿè®¡ä¿¡æ¯ï¼š",
                    df[numeric_cols].describe().to_string()
                ])
            
            preview_text = "\n".join(preview_info)
            
            # å¦‚æœé¢„è§ˆæ–‡æœ¬å¤ªé•¿ï¼Œè¿›è¡Œæˆªæ–­
            if len(preview_text) > self.max_preview_size:
                preview_text = preview_text[:self.max_preview_size] + "..."
            
            return preview_text
            
        except Exception as e:
            raise ValueError(f"è¯»å–æ•°æ®é›†æ—¶å‡ºé”™ï¼š{str(e)}")

    def _get_file_type(self, file_path: str) -> str:
        """è·å–æ–‡ä»¶ç±»å‹"""
        ext = os.path.splitext(file_path)[1].lower()
        supported_types = {
            '.csv': 'csv',
            '.xlsx': 'excel',
            '.xls': 'excel'
        }
        
        if ext not in supported_types:
            raise ValueError(f"ä¸æ”¯æŒçš„æ–‡ä»¶ç±»å‹ï¼š{ext}")
        
        return supported_types[ext]

    def _get_llm_analysis(self, dataset_preview: str) -> str:
        """è·å– LLM åˆ†æç»“æœ"""
        try:
            print("æ­£åœ¨å‘é€è¯·æ±‚åˆ° LLM...")
            response = self.llm.chat.completions.create(
                model=self.model,
                messages=[{
                    "role": "user",
                    "content": get_analysis_prompt(dataset_preview)
                }]
            )
            print("LLM å“åº”æ¥æ”¶å®Œæˆ")
            return response.choices[0].message.content
        except Exception as e:
            print(f"LLM è°ƒç”¨å‡ºé”™: {str(e)}")
            raise LLMError(f"LLM è°ƒç”¨å¤±è´¥: {str(e)}")

    def _parse_analysis_response(self, response_text: str) -> Dict[str, Any]:
        """è§£æ LLM çš„åˆ†æç»“æœ"""
        print("å¼€å§‹è§£æ LLM å“åº”...")
        print(f"å“åº”å†…å®¹: {response_text[:200]}...")  # åªæ‰“å°å‰200ä¸ªå­—ç¬¦
        
        # æ¸…ç†å“åº”æ–‡æœ¬ï¼Œç§»é™¤ Markdown ä»£ç å—æ ‡è®°
        def clean_json_response(text: str) -> str:
            # ç§»é™¤ ```json å’Œ ``` æ ‡è®°
            text = text.replace("```json", "").replace("```", "")
            # ç§»é™¤å¼€å¤´å’Œç»“å°¾çš„ç©ºç™½å­—ç¬¦
            text = text.strip()
            return text
        
        try:
            # æ¸…ç†å¹¶å°è¯•è§£æ JSON
            print("å°è¯•è§£æ JSON...")
            cleaned_response = clean_json_response(response_text)
            result = json.loads(cleaned_response)
            print("JSON è§£ææˆåŠŸ")
            
            # éªŒè¯å¿…è¦çš„å­—æ®µ
            required_fields = ['data_type', 'task_type', 'features', 'target', 'preprocessing_steps']
            missing_fields = [field for field in required_fields if field not in result]
            
            if missing_fields:
                print(f"ç¼ºå°‘å¿…è¦å­—æ®µ: {missing_fields}")
                raise ValueError(f"åˆ†æç»“æœç¼ºå°‘å¿…è¦å­—æ®µï¼š{', '.join(missing_fields)}")
            
            print("æ‰€æœ‰å¿…è¦å­—æ®µéƒ½å­˜åœ¨")
            
            # æ ‡å‡†åŒ–ç»“æœæ ¼å¼
            standardized_result = {
                'data_type': result['data_type'],
                'task_type': result['task_type'],
                'features': result['features'],
                'target': result['target'],
                'preprocessing_steps': result['preprocessing_steps']
            }
            
            print("ç»“æœæ ‡å‡†åŒ–å®Œæˆ")
            return standardized_result
            
        except json.JSONDecodeError as e:
            print(f"JSON è§£æå¤±è´¥: {str(e)}")
            # å¦‚æœä¸æ˜¯æœ‰æ•ˆçš„ JSONï¼Œå°è¯•æå–å…³é”®ä¿¡æ¯
            try:
                print("å°è¯•é‡æ–°æ ¼å¼åŒ–å“åº”...")
                # é‡æ–°è¯·æ±‚ LLM æ ¼å¼åŒ–ç»“æœ
                format_prompt = f"""
                è¯·å°†ä»¥ä¸‹åˆ†æç»“æœè½¬æ¢ä¸ºæ ‡å‡†çš„ JSON æ ¼å¼ï¼ˆä¸è¦æ·»åŠ ä»»ä½• Markdown æ ‡è®°ï¼‰ï¼š
                {response_text}
                
                éœ€è¦åŒ…å«ä»¥ä¸‹å­—æ®µï¼š
                - data_type: æ•°æ®é›†ç±»å‹
                - task_type: ä»»åŠ¡ç±»å‹
                - features: ç‰¹å¾è¯´æ˜
                - target: ç›®æ ‡å˜é‡
                - preprocessing_steps: é¢„å¤„ç†æ­¥éª¤
                
                ç›´æ¥è¿”å› JSON å¯¹è±¡ï¼Œä¸è¦æ·»åŠ ä»»ä½•å…¶ä»–æ ‡è®°ã€‚
                """
                
                format_response = self.llm.chat.completions.create(
                    model=self.model,
                    messages=[{"role": "user", "content": format_prompt}]
                )
                
                print("é‡æ–°æ ¼å¼åŒ–å®Œæˆï¼Œå°è¯•è§£ææ–°çš„å“åº”...")
                return self._parse_analysis_response(format_response.choices[0].message.content)
                
            except Exception as e:
                print(f"é‡æ–°æ ¼å¼åŒ–å¤±è´¥: {str(e)}")
                raise ValueError(f"æ— æ³•è§£æåˆ†æç»“æœï¼š{str(e)}")

    @RetryHandler().retry_on_exception
    def analyze_dataset(self, dataset_path: str) -> Dict[str, Any]:
        """ä½¿ç”¨ LLM åˆ†ææ•°æ®é›†"""
        try:
            print("å¼€å§‹è¯»å–æ•°æ®é›†...")
            dataset_preview = self._get_dataset_preview(dataset_path)
            print("æ•°æ®é›†é¢„è§ˆè·å–å®Œæˆï¼Œæ­£åœ¨è°ƒç”¨ LLM åˆ†æ...")
            response = self._get_llm_analysis(dataset_preview)
            print("LLM åˆ†æå®Œæˆï¼Œæ­£åœ¨è§£æç»“æœ...")
            return self._parse_analysis_response(response)
        except Exception as e:
            self.logger.error(f"æ•°æ®é›†åˆ†æå¤±è´¥: {str(e)}")
            raise LLMError(f"æ•°æ®é›†åˆ†æå¤±è´¥: {str(e)}")

class ContentGenerator:
    """å†…å®¹ç”Ÿæˆç»„ä»¶"""
    def __init__(self, llm_client: OpenAI, *, config=None):
        self.llm = llm_client
        self.retry_handler = RetryHandler()
        self.logger = logging.getLogger(__name__)
        self.config = config
        self.model = config.llm.model if config else "gpt-4"
    
    def _get_llm_response(self, prompt: str) -> str:
        """è·å– LLM å“åº”"""
        try:
            response = self.llm.chat.completions.create(
                model=self.model,
                messages=[{"role": "user", "content": prompt}]
            )
            return response.choices[0].message.content
        except Exception as e:
            raise LLMError(f"LLM è°ƒç”¨å¤±è´¥: {str(e)}")

    def _validate_theory_content(self, content: str) -> None:
        """éªŒè¯ç†è®ºå†…å®¹"""
        if not content or len(content.strip()) < 100:
            raise ValidationError("ç†è®ºå†…å®¹è¿‡çŸ­æˆ–ä¸ºç©º")
        
        # æ”¯æŒå¤šç§æ ‡é¢˜æ ¼å¼å’Œå˜ä½“
        title_patterns = {
            "ä»»åŠ¡å®šä¹‰": [
                "# ä»»åŠ¡å®šä¹‰", "## ä»»åŠ¡å®šä¹‰", "### ä»»åŠ¡å®šä¹‰",
                "# ä»»åŠ¡å®šä¹‰ä¸ç›®æ ‡", "## ä»»åŠ¡å®šä¹‰ä¸ç›®æ ‡", "### ä»»åŠ¡å®šä¹‰ä¸ç›®æ ‡",
                "ä»»åŠ¡å®šä¹‰", "ä»»åŠ¡å®šä¹‰ä¸ç›®æ ‡"
            ],
            "æ•°æ®é›†ä»‹ç»": [
                "# æ•°æ®é›†ä»‹ç»", "## æ•°æ®é›†ä»‹ç»", "### æ•°æ®é›†ä»‹ç»",
                "# æ•°æ®é›†èƒŒæ™¯", "## æ•°æ®é›†èƒŒæ™¯", "### æ•°æ®é›†èƒŒæ™¯",
                "# æ•°æ®é›†è¯´æ˜", "## æ•°æ®é›†è¯´æ˜", "### æ•°æ®é›†è¯´æ˜",
                "æ•°æ®é›†ä»‹ç»", "æ•°æ®é›†èƒŒæ™¯", "æ•°æ®é›†è¯´æ˜",
                "1. æ•°æ®é›†èƒŒæ™¯", "2. å­—æ®µè¯´æ˜", "3. æ•°æ®è´¨é‡è¯„ä¼°", "4. æ•°æ®é›†ç‰¹ç‚¹å’Œä»·å€¼"
            ],
            "æ•°æ®æ¢ç´¢": [
                "# æ•°æ®æ¢ç´¢", "## æ•°æ®æ¢ç´¢", "### æ•°æ®æ¢ç´¢",
                "# æ•°æ®æ¢ç´¢ä¸åˆ†æ", "## æ•°æ®æ¢ç´¢ä¸åˆ†æ", "### æ•°æ®æ¢ç´¢ä¸åˆ†æ",
                "# æ¢ç´¢æ€§åˆ†æ", "## æ¢ç´¢æ€§åˆ†æ", "### æ¢ç´¢æ€§åˆ†æ",
                "æ•°æ®æ¢ç´¢", "æ•°æ®æ¢ç´¢ä¸åˆ†æ", "æ¢ç´¢æ€§åˆ†æ",
                "1. æ•°æ®åˆ†å¸ƒåˆ†æ", "2. ç‰¹å¾ç›¸å…³æ€§åˆ†æ", "3. å¼‚å¸¸å€¼æ£€æµ‹", "4. ç¼ºå¤±å€¼åˆ†æ"
            ],
            "ç‰¹å¾å·¥ç¨‹": [
                "# ç‰¹å¾å·¥ç¨‹", "## ç‰¹å¾å·¥ç¨‹", "### ç‰¹å¾å·¥ç¨‹",
                "# ç‰¹å¾å·¥ç¨‹ä¸é¢„å¤„ç†", "## ç‰¹å¾å·¥ç¨‹ä¸é¢„å¤„ç†", "### ç‰¹å¾å·¥ç¨‹ä¸é¢„å¤„ç†",
                "ç‰¹å¾å·¥ç¨‹", "ç‰¹å¾å·¥ç¨‹ä¸é¢„å¤„ç†",
                "1. æ•°æ®é¢„å¤„ç†", "2. ç‰¹å¾é€‰æ‹©", "3. ç‰¹å¾æ„é€ ", "4. ç‰¹å¾ç¼–ç "
            ],
            "æ¨¡å‹æ„å»º": [
                "# æ¨¡å‹æ„å»º", "## æ¨¡å‹æ„å»º", "### æ¨¡å‹æ„å»º",
                "# æ¨¡å‹æ„å»ºä¸è®­ç»ƒ", "## æ¨¡å‹æ„å»ºä¸è®­ç»ƒ", "### æ¨¡å‹æ„å»ºä¸è®­ç»ƒ",
                "# ç®—æ³•å®ç°", "## ç®—æ³•å®ç°", "### ç®—æ³•å®ç°",
                "æ¨¡å‹æ„å»º", "æ¨¡å‹æ„å»ºä¸è®­ç»ƒ", "ç®—æ³•å®ç°",
                "1. ç®—æ³•åŸºç¡€", "2. å‚æ•°è¯¦è§£", "3. æ¨¡å‹è®­ç»ƒä¸è¯„ä¼°", "4. æ¨¡å‹ä¼˜åŒ–", "5. æ¨¡å‹åº”ç”¨"
            ]
        }
        
        # æ£€æŸ¥æ¯ä¸ªå¿…éœ€éƒ¨åˆ†
        missing_sections = []
        for section, patterns in title_patterns.items():
            if not any(pattern in content for pattern in patterns):
                missing_sections.append(section)
        
        # å¦‚æœå†…å®¹é•¿åº¦è¶³å¤Ÿä¸”åŒ…å«ä»£ç ç¤ºä¾‹ï¼Œæˆ‘ä»¬å¯ä»¥é€‚å½“æ”¾å®½è¦æ±‚
        # åªè¦æ±‚åŒ…å«ä»»åŠ¡å®šä¹‰ã€æ•°æ®é›†ä»‹ç»å’Œæ¨¡å‹æ„å»ºè¿™ä¸‰ä¸ªæ ¸å¿ƒéƒ¨åˆ†
        if len(content) >= 2000 and "```python" in content:
            core_sections = ["ä»»åŠ¡å®šä¹‰", "æ•°æ®é›†ä»‹ç»", "æ¨¡å‹æ„å»º"]
            missing_sections = [s for s in missing_sections if s in core_sections]
        
        if missing_sections:
            raise ValidationError(f"ç†è®ºå†…å®¹ç¼ºå°‘ä»¥ä¸‹éƒ¨åˆ†: {', '.join(missing_sections)}")
        
        # æ£€æŸ¥å†…å®¹é•¿åº¦æ˜¯å¦åˆé€‚
        if len(content) < 1500:  # ä¿æŒæœ€å°é•¿åº¦è¦æ±‚
            raise ValidationError("ç†è®ºå†…å®¹é•¿åº¦ä¸è¶³ï¼Œè¯·ç¡®ä¿å†…å®¹å……åˆ†è¯¦ç»†")
    
    def _validate_code_content(self, content: str) -> None:
        """éªŒè¯ä»£ç å†…å®¹"""
        if not content or "```python" not in content:
            raise ValidationError("ä»£ç å†…å®¹æ— æ•ˆ")
        
        # æ£€æŸ¥æ˜¯å¦åŒ…å«å¿…è¦çš„å¯¼å…¥è¯­å¥
        required_imports = ["pandas", "numpy", "sklearn"]
        for imp in required_imports:
            if imp not in content.lower():
                raise ValidationError(f"ä»£ç ç¼ºå°‘ {imp} åº“çš„å¯¼å…¥")
    
    def _validate_exercises_content(self, content: str) -> None:
        """éªŒè¯ç»ƒä¹ é¢˜å†…å®¹"""
        if not content or len(content.strip()) < 100:
            raise ValidationError("ç»ƒä¹ é¢˜å†…å®¹è¿‡çŸ­æˆ–ä¸ºç©º")

        # æ£€æŸ¥ç»ƒä¹ é¢˜æ•°é‡
        exercise_count = content.count("## ç»ƒä¹ ")
        if exercise_count < 3:
            raise ValidationError("ç»ƒä¹ é¢˜æ•°é‡ä¸è¶³ï¼Œè‡³å°‘éœ€è¦3ä¸ªç»ƒä¹ é¢˜")

        # æ£€æŸ¥æ¯ä¸ªç»ƒä¹ é¢˜çš„ç»“æ„
        exercises = content.split("## ç»ƒä¹ ")[1:]  # è·³è¿‡ç¬¬ä¸€ä¸ªç©ºå­—ç¬¦ä¸²
        for i, exercise in enumerate(exercises, 1):
            if "### é—®é¢˜" not in exercise:
                raise ValidationError(f"ç»ƒä¹  {i} ç¼ºå°‘é—®é¢˜æè¿°")
            if "### å‚è€ƒç­”æ¡ˆ" not in exercise:
                raise ValidationError(f"ç»ƒä¹  {i} ç¼ºå°‘å‚è€ƒç­”æ¡ˆ")
            if len(exercise.strip()) < 50:
                raise ValidationError(f"ç»ƒä¹  {i} å†…å®¹è¿‡çŸ­")

    @RetryHandler().retry_on_exception
    def generate_theory(self, knowledge_points: List[str], dataset_info: Dict[str, Any]) -> str:
        """åˆ†æ­¥ç”Ÿæˆç†è®ºè§£é‡Šå†…å®¹"""
        try:
            parts = []
            
            # 1. ç”Ÿæˆä»»åŠ¡å®šä¹‰éƒ¨åˆ†
            task_prompt = f"""
åŸºäºä»¥ä¸‹çŸ¥è¯†ç‚¹å’Œæ•°æ®é›†ï¼Œç”Ÿæˆä»»åŠ¡å®šä¹‰éƒ¨åˆ†çš„å†…å®¹ï¼š

çŸ¥è¯†ç‚¹ï¼š
{', '.join(knowledge_points)}

æ•°æ®é›†ä¿¡æ¯ï¼š
{json.dumps(dataset_info, ensure_ascii=False, indent=2)}

è¯·åŒ…å«ä»¥ä¸‹å†…å®¹ï¼š
1. ä»»åŠ¡ç±»å‹è¯´æ˜ï¼ˆç»“åˆçŸ¥è¯†ç‚¹å’Œæ•°æ®é›†ç‰¹ç‚¹ï¼‰
2. ä»»åŠ¡éš¾ç‚¹å’ŒæŒ‘æˆ˜ï¼ˆé’ˆå¯¹å…·ä½“æ•°æ®é›†ï¼‰
3. è¯„ä¼°æŒ‡æ ‡çš„é€‰æ‹©å’Œè§£é‡Šï¼ˆé€‚åˆè¯¥ä»»åŠ¡çš„æŒ‡æ ‡ï¼‰
4. é¢„æœŸç›®æ ‡å’Œå®é™…åº”ç”¨ä»·å€¼ï¼ˆåœ¨è¯¥åœºæ™¯ä¸‹çš„åº”ç”¨ï¼‰
"""
            task_response = self._get_llm_response(task_prompt)
            parts.append(task_response)
            
            # 2. ç”Ÿæˆæ•°æ®é›†ä»‹ç»éƒ¨åˆ†
            dataset_prompt = f"""
åŸºäºä»¥ä¸‹çŸ¥è¯†ç‚¹å’Œæ•°æ®é›†ï¼Œè¯¦ç»†ä»‹ç»æ•°æ®é›†ï¼š

çŸ¥è¯†ç‚¹ï¼š
{', '.join(knowledge_points)}

æ•°æ®é›†ä¿¡æ¯ï¼š
{json.dumps(dataset_info, ensure_ascii=False, indent=2)}

è¯·åŒ…å«ä»¥ä¸‹å†…å®¹ï¼š
1. æ•°æ®é›†èƒŒæ™¯ï¼š
   - æ•°æ®æ¥æºå’Œæ”¶é›†è¿‡ç¨‹
   - æ•°æ®è§„æ¨¡å’ŒèŒƒå›´
   - åœ¨{', '.join(knowledge_points)}ä¸­çš„åº”ç”¨åœºæ™¯
2. å­—æ®µè¯´æ˜ï¼š
   - æ¯ä¸ªå­—æ®µçš„å…·ä½“å«ä¹‰
   - æ•°æ®ç±»å‹å’Œå–å€¼èŒƒå›´
   - ä¸{', '.join(knowledge_points)}çš„å…³è”
   - ä¸šåŠ¡æ„ä¹‰å’Œé‡è¦æ€§
3. æ•°æ®è´¨é‡è¯„ä¼°ï¼š
   - å®Œæ•´æ€§å’Œå‡†ç¡®æ€§åˆ†æ
   - æ•°æ®åˆ†å¸ƒç‰¹ç‚¹
   - æ½œåœ¨çš„è´¨é‡é—®é¢˜
4. æ•°æ®é›†ç‰¹ç‚¹å’Œä»·å€¼ï¼š
   - å¯¹å­¦ä¹ {', '.join(knowledge_points)}çš„å¸®åŠ©
   - å®é™…åº”ç”¨ä¸­çš„ä»·å€¼
"""
            dataset_response = self._get_llm_response(dataset_prompt)
            parts.append(dataset_response)
            
            # 3. ç”Ÿæˆæ•°æ®æ¢ç´¢ä¸åˆ†æéƒ¨åˆ†
            exploration_prompt = f"""
åŸºäºä»¥ä¸‹çŸ¥è¯†ç‚¹å’Œæ•°æ®é›†ï¼Œç”Ÿæˆæ•°æ®æ¢ç´¢ä¸åˆ†æå†…å®¹ï¼š

çŸ¥è¯†ç‚¹ï¼š
{', '.join(knowledge_points)}

æ•°æ®é›†ä¿¡æ¯ï¼š
{json.dumps(dataset_info, ensure_ascii=False, indent=2)}

è¯·åŒ…å«ä»¥ä¸‹å†…å®¹ï¼š
1. æ•°æ®åˆ†å¸ƒåˆ†æï¼š
   - é‡è¦ç‰¹å¾çš„åˆ†å¸ƒæƒ…å†µ
   - ä¸{', '.join(knowledge_points)}ç›¸å…³çš„ç‰¹å¾åˆ†æ
   - é™„å¸¦å¯è§†åŒ–ä»£ç 
2. ç‰¹å¾ç›¸å…³æ€§åˆ†æï¼š
   - ç‰¹å¾é—´çš„å…³è”å…³ç³»
   - ä¸ç›®æ ‡å˜é‡çš„ç›¸å…³æ€§
   - é™„å¸¦ç›¸å…³æ€§åˆ†æä»£ç 
3. å¼‚å¸¸å€¼æ£€æµ‹ï¼š
   - åŸºäºé¢†åŸŸçŸ¥è¯†çš„å¼‚å¸¸å®šä¹‰
   - å¼‚å¸¸å€¼æ£€æµ‹æ–¹æ³•
   - é™„å¸¦æ£€æµ‹ä»£ç 
4. ç¼ºå¤±å€¼åˆ†æï¼š
   - ç¼ºå¤±å€¼åˆ†å¸ƒæƒ…å†µ
   - å¯¹{', '.join(knowledge_points)}çš„å½±å“
   - é™„å¸¦å¤„ç†ä»£ç 
"""
            exploration_response = self._get_llm_response(exploration_prompt)
            parts.append(exploration_response)
            
            # 4. ç”Ÿæˆç‰¹å¾å·¥ç¨‹éƒ¨åˆ†
            feature_prompt = f"""
åŸºäºä»¥ä¸‹çŸ¥è¯†ç‚¹å’Œæ•°æ®é›†ï¼Œç”Ÿæˆç‰¹å¾å·¥ç¨‹å†…å®¹ï¼Œæ³¨æ„è¦å¾ªåºæ¸è¿›ï¼Œæ¯ä¸ªæ­¥éª¤éƒ½è¦é…åˆä»£ç ç¤ºä¾‹ï¼š

çŸ¥è¯†ç‚¹ï¼š
{', '.join(knowledge_points)}

æ•°æ®é›†ä¿¡æ¯ï¼š
{json.dumps(dataset_info, ensure_ascii=False, indent=2)}

èƒŒæ™¯è¯´æ˜ï¼š
1. æœ¬æ•™ç¨‹æ—¨åœ¨è®²è§£{', '.join(knowledge_points)}ç›¸å…³çš„ç‰¹å¾å·¥ç¨‹æ–¹æ³•
2. ä½¿ç”¨{dataset_info['data_type']}ç±»å‹çš„æ•°æ®é›†
3. é’ˆå¯¹{dataset_info['task_type']}ä»»åŠ¡è¿›è¡Œç‰¹å¾å¤„ç†

è¯·æŒ‰ä»¥ä¸‹æ­¥éª¤ç»„ç»‡å†…å®¹ï¼š

1. æ•°æ®é¢„å¤„ç†ï¼š
   a. ç»“åˆçŸ¥è¯†ç‚¹ï¼Œè§£é‡Šä¸ºä»€ä¹ˆéœ€è¦è¿›è¡Œé¢„å¤„ç†
   b. å±•ç¤ºæ•°æ®çš„åˆå§‹çŠ¶æ€ï¼Œé‡ç‚¹å…³æ³¨ä¸{', '.join(knowledge_points)}ç›¸å…³çš„ç‰¹å¾
   ```python
   # å±•ç¤ºä»£ç 
   ```
   c. åŸºäºæ•°æ®ç‰¹ç‚¹ï¼Œå¤„ç†ç¼ºå¤±å€¼å’Œå¼‚å¸¸å€¼
   ```python
   # å¤„ç†ä»£ç 
   ```
   d. åˆ†æå¤„ç†ç»“æœå¯¹åç»­å»ºæ¨¡çš„å½±å“

2. ç‰¹å¾é€‰æ‹©ï¼š
   a. ç»“åˆ{', '.join(knowledge_points)}ï¼Œè§£é‡Šç‰¹å¾é€‰æ‹©çš„é‡è¦æ€§
   b. åˆ†æå„ç‰¹å¾å¯¹{dataset_info['target']}çš„å½±å“
   ```python
   # åˆ†æä»£ç 
   ```
   c. ä½¿ç”¨åˆé€‚çš„ç‰¹å¾é€‰æ‹©æ–¹æ³•
   ```python
   # é€‰æ‹©ä»£ç 
   ```
   d. è¯„ä¼°ç‰¹å¾é€‰æ‹©çš„æ•ˆæœ

3. ç‰¹å¾æ„é€ ï¼š
   a. åŸºäº{', '.join(knowledge_points)}çš„åŸç†ï¼Œè®¾è®¡æ–°ç‰¹å¾
   b. è§£é‡Šæ¯ä¸ªæ–°ç‰¹å¾çš„æ„é€ æ€è·¯
   ```python
   # æ„é€ ä»£ç 
   ```
   c. éªŒè¯æ–°ç‰¹å¾çš„æœ‰æ•ˆæ€§
   ```python
   # éªŒè¯ä»£ç 
   ```
   d. åˆ†ææ–°ç‰¹å¾å¯¹æ¨¡å‹çš„è´¡çŒ®

4. ç‰¹å¾ç¼–ç ï¼š
   a. é’ˆå¯¹{dataset_info['data_type']}æ•°æ®çš„ç‰¹ç‚¹ï¼Œé€‰æ‹©ç¼–ç æ–¹æ³•
   b. å®ç°ç‰¹å¾ç¼–ç 
   ```python
   # ç¼–ç ä»£ç 
   ```
   c. æ£€æŸ¥ç¼–ç ç»“æœ
   ```python
   # æ£€æŸ¥ä»£ç 
   ```
   d. è¯„ä¼°ç¼–ç æ•ˆæœ

æ¯ä¸ªæ­¥éª¤éƒ½è¦ï¼š
1. å…ˆè§£é‡ŠåŸç†å’Œç›®çš„ï¼Œè¦ç»“åˆå…·ä½“çš„çŸ¥è¯†ç‚¹
2. å±•ç¤ºä»£ç å®ç°ï¼Œå¹¶è§£é‡Šå…³é”®æ­¥éª¤
3. åˆ†æå¤„ç†ç»“æœ
4. æ€»ç»“ç»éªŒï¼Œç‰¹åˆ«æ˜¯ä¸{', '.join(knowledge_points)}ç›¸å…³çš„éƒ¨åˆ†
"""
            feature_response = self._get_llm_response(feature_prompt)
            parts.append(feature_response)
            
            # 5. ç”Ÿæˆæ¨¡å‹æ„å»ºéƒ¨åˆ†
            model_prompt = f"""
åŸºäºä»¥ä¸‹çŸ¥è¯†ç‚¹å’Œæ•°æ®é›†ï¼Œç”Ÿæˆæ¨¡å‹æ„å»ºå†…å®¹ï¼Œè¦å¾ªåºæ¸è¿›åœ°å¸¦é¢†å­¦ç”Ÿå®è·µï¼š

çŸ¥è¯†ç‚¹ï¼š
{', '.join(knowledge_points)}

æ•°æ®é›†ä¿¡æ¯ï¼š
{json.dumps(dataset_info, ensure_ascii=False, indent=2)}

èƒŒæ™¯è¯´æ˜ï¼š
1. æœ¬æ•™ç¨‹ä¸»è¦è®²è§£{', '.join(knowledge_points)}çš„åŸç†å’Œåº”ç”¨
2. ä½¿ç”¨{dataset_info['data_type']}ç±»å‹çš„æ•°æ®é›†
3. é’ˆå¯¹{dataset_info['task_type']}ä»»åŠ¡è¿›è¡Œå»ºæ¨¡

è¯·æŒ‰ä»¥ä¸‹æ­¥éª¤ç»„ç»‡å†…å®¹ï¼š

1. ç®—æ³•åŸºç¡€ï¼š
   a. ç›´è§‚è§£é‡Š{', '.join(knowledge_points)}çš„åŸºæœ¬åŸç†
   b. æ ¸å¿ƒæ¦‚å¿µè§£é‡Šï¼š
      - ç®—æ³•çš„ä¸»è¦ç»„æˆéƒ¨åˆ†
      - å…³é”®å‚æ•°å’Œè¶…å‚æ•°
      - ä¼˜åŒ–ç›®æ ‡å’ŒæŸå¤±å‡½æ•°
   c. å®ç°ä¸€ä¸ªç®€å•çš„ç¤ºä¾‹
   ```python
   # åŸºç¡€å®ç°ä»£ç 
   ```
   d. åˆ†æè¿è¡Œç»“æœ

2. å‚æ•°è¯¦è§£ï¼š
   a. ç»“åˆ{dataset_info['task_type']}ä»»åŠ¡ï¼Œè§£é‡Šå„å‚æ•°çš„ä½œç”¨
   b. è¿›è¡Œå‚æ•°å®éªŒ
   ```python
   # å‚æ•°å®éªŒä»£ç 
   ```
   c. å¯è§†åŒ–ä¸åŒå‚æ•°çš„æ•ˆæœ
   ```python
   # å¯è§†åŒ–ä»£ç 
   ```
   d. æ€»ç»“å‚æ•°è°ƒä¼˜ç»éªŒ

3. æ¨¡å‹è®­ç»ƒä¸è¯„ä¼°ï¼š
   a. è®¾è®¡è®­ç»ƒæµç¨‹
   b. å®ç°äº¤å‰éªŒè¯
   ```python
   # è®­ç»ƒä»£ç 
   ```
   c. ä½¿ç”¨å¤šç§è¯„ä¼°æŒ‡æ ‡
   ```python
   # è¯„ä¼°ä»£ç 
   ```
   d. åˆ†ææ¨¡å‹è¡¨ç°

4. æ¨¡å‹ä¼˜åŒ–ï¼š
   a. è¯Šæ–­æ¨¡å‹é—®é¢˜
   b. å®ç°ä¼˜åŒ–æ–¹æ³•
   ```python
   # ä¼˜åŒ–ä»£ç 
   ```
   c. å¯¹æ¯”ä¼˜åŒ–æ•ˆæœ
   ```python
   # å¯¹æ¯”ä»£ç 
   ```
   d. æ€»ç»“ä¼˜åŒ–ç»éªŒ

5. æ¨¡å‹åº”ç”¨ï¼š
   a. ç»“åˆå®é™…åœºæ™¯ï¼Œè¯´æ˜åº”ç”¨æ–¹æ³•
   b. å®ç°é¢„æµ‹æµç¨‹
   ```python
   # åº”ç”¨ä»£ç 
   ```
   c. å¯è§†åŒ–é¢„æµ‹ç»“æœ
   ```python
   # å¯è§†åŒ–ä»£ç 
   ```
   d. ç»™å‡ºå®è·µå»ºè®®

æ¯ä¸ªæ­¥éª¤éƒ½è¦ï¼š
1. é€šä¿—æ˜“æ‡‚åœ°è§£é‡ŠåŸç†ï¼Œè¦ç»“åˆå…·ä½“çŸ¥è¯†ç‚¹
2. æä¾›å®Œæ•´çš„ä»£ç ç¤ºä¾‹
3. å±•ç¤ºå’Œåˆ†æè¿è¡Œç»“æœ
4. æ€»ç»“å­¦ä¹ è¦ç‚¹
5. æä¾›æ€è€ƒé¢˜ï¼ŒåŠ æ·±ç†è§£

æ³¨æ„ï¼š
1. ä»£ç è¦å¾ªåºæ¸è¿›ï¼Œç”±ç®€å•åˆ°å¤æ‚
2. æ¯ä¸ªæ­¥éª¤éƒ½è¦æœ‰æ˜ç¡®çš„å­¦ä¹ ç›®æ ‡
3. è¦æœ‰é€‚å½“çš„äº¤äº’æ€§ï¼Œè®©å­¦ç”Ÿèƒ½è·Ÿç€åš
4. ç»“åˆæ•°æ®é›†ç‰¹ç‚¹æ¥è®²è§£
5. çªå‡º{', '.join(knowledge_points)}çš„å…³é”®çŸ¥è¯†ç‚¹
"""
            model_response = self._get_llm_response(model_prompt)
            parts.append(model_response)
            
            # åˆå¹¶æ‰€æœ‰éƒ¨åˆ†
            complete_content = "\n\n".join(parts)
            
            # éªŒè¯å®Œæ•´å†…å®¹
            self._validate_theory_content(complete_content)
            return complete_content
            
        except Exception as e:
            self.logger.error(f"ç†è®ºå†…å®¹ç”Ÿæˆå¤±è´¥: {str(e)}")
            raise LLMError(f"ç†è®ºå†…å®¹ç”Ÿæˆå¤±è´¥: {str(e)}")
    
    @RetryHandler().retry_on_exception
    def generate_code_examples(self, dataset_info: Dict[str, Any]) -> str:
        """ç”Ÿæˆä»£ç ç¤ºä¾‹"""
        try:
            response = self._get_llm_response(
                get_code_prompt(dataset_info)
            )
            self._validate_code_content(response)
            return response
        except Exception as e:
            self.logger.error(f"ä»£ç ç¤ºä¾‹ç”Ÿæˆå¤±è´¥: {str(e)}")
            raise LLMError(f"ä»£ç ç¤ºä¾‹ç”Ÿæˆå¤±è´¥: {str(e)}")
    
    @RetryHandler().retry_on_exception
    def generate_exercises(self, difficulty_level: str, dataset_info: Dict[str, Any]) -> str:
        """ç”Ÿæˆç»ƒä¹ é¢˜"""
        try:
            response = self._get_llm_response(
                get_exercises_prompt(difficulty_level, dataset_info)
            )
            self._validate_exercises_content(response)
            return response
        except Exception as e:
            self.logger.error(f"ç»ƒä¹ é¢˜ç”Ÿæˆå¤±è´¥: {str(e)}")
            raise LLMError(f"ç»ƒä¹ é¢˜ç”Ÿæˆå¤±è´¥: {str(e)}")

class NotebookAssembler:
    """Notebook ç»„è£…ç»„ä»¶"""
    def __init__(self, llm_client: OpenAI, *, config=None):
        self.llm = llm_client
        self.retry_handler = RetryHandler()
        self.logger = logging.getLogger(__name__)
        self.config = config
        self.model = config.llm.model if config else "gpt-4"
        self.notebook_template = {
            "cells": [],
            "metadata": {
                "kernelspec": {
                    "display_name": "Python 3",
                    "language": "python",
                    "name": "python3"
                },
                "language_info": {
                    "codemirror_mode": {
                        "name": "ipython",
                        "version": 3
                    },
                    "file_extension": ".py",
                    "mimetype": "text/x-python",
                    "name": "python",
                    "nbconvert_exporter": "python",
                    "pygments_lexer": "ipython3",
                    "version": "3.8.0"
                }
            },
            "nbformat": 4,
            "nbformat_minor": 4
        }
    
    def _organize_content(self, content: Dict[str, Any]) -> str:
        """åˆ†æ®µç»„ç»‡å†…å®¹çš„é¡ºåºå’Œç»“æ„"""
        organized_parts = []
        
        # 1. å¤„ç†ç†è®ºå†…å®¹
        if "theory" in content:
            theory_prompt = f"""
è¯·ä¼˜åŒ–ä»¥ä¸‹ç†è®ºå†…å®¹çš„ç»“æ„å’Œç»„ç»‡ï¼Œç¡®ä¿å±‚æ¬¡æ¸…æ™°ï¼Œä¿æŒæ‰€æœ‰å†…å®¹å®Œæ•´ã€‚
ä¸»è¦å…³æ³¨ï¼š
1. æ ‡é¢˜å±‚çº§çš„åˆç†æ€§
2. å†…å®¹çš„è¿è´¯æ€§
3. ä¿æŒæ‰€æœ‰ä»£ç ç¤ºä¾‹
4. ç¡®ä¿æ•°æ®é›†ä»‹ç»ã€ä»»åŠ¡å®šä¹‰ç­‰æ ¸å¿ƒéƒ¨åˆ†å®Œæ•´

å†…å®¹å¦‚ä¸‹ï¼š
{content['theory']}
"""
            try:
                theory_response = self.llm.chat.completions.create(
                    model=self.model,
                    messages=[{"role": "user", "content": theory_prompt}]
                )
                organized_parts.append(theory_response.choices[0].message.content)
            except Exception as e:
                self.logger.warning(f"ç†è®ºå†…å®¹ä¼˜åŒ–å¤±è´¥ï¼Œä½¿ç”¨åŸå†…å®¹: {str(e)}")
                organized_parts.append(content["theory"])
        
        # 2. å¤„ç†ä»£ç ç¤ºä¾‹ï¼ˆå¦‚æœæ˜¯ç‹¬ç«‹çš„ï¼‰
        if "code" in content and isinstance(content["code"], str):
            code_prompt = f"""
è¯·ä¼˜åŒ–ä»¥ä¸‹ä»£ç ç¤ºä¾‹çš„ç»„ç»‡ï¼Œç¡®ä¿ï¼š
1. ä»£ç é€»è¾‘æ¸…æ™°
2. æ³¨é‡Šå®Œæ•´
3. æ¯ä¸ªæ­¥éª¤éƒ½æœ‰å……åˆ†è¯´æ˜
4. ä¿æŒæ‰€æœ‰åŠŸèƒ½å®Œæ•´

ä»£ç ç¤ºä¾‹ï¼š
{content['code']}
"""
            try:
                code_response = self.llm.chat.completions.create(
                    model=self.model,
                    messages=[{"role": "user", "content": code_prompt}]
                )
                organized_parts.append(code_response.choices[0].message.content)
            except Exception as e:
                self.logger.warning(f"ä»£ç ç¤ºä¾‹ä¼˜åŒ–å¤±è´¥ï¼Œä½¿ç”¨åŸå†…å®¹: {str(e)}")
                organized_parts.append(content["code"])
        
        # 3. å¤„ç†ç»ƒä¹ å†…å®¹
        if "exercises" in content:
            exercises_prompt = f"""
è¯·ä¼˜åŒ–ä»¥ä¸‹ç»ƒä¹ å†…å®¹çš„ç»„ç»‡ï¼Œç¡®ä¿ï¼š
1. ç»ƒä¹ é¢˜çš„éš¾åº¦é€’è¿›
2. é—®é¢˜æè¿°æ¸…æ™°
3. ç­”æ¡ˆè§£é‡Šè¯¦ç»†
4. ä¿æŒæ‰€æœ‰ç»ƒä¹ é¢˜å®Œæ•´

ç»ƒä¹ å†…å®¹ï¼š
{content['exercises']}
"""
            try:
                exercises_response = self.llm.chat.completions.create(
                    model=self.model,
                    messages=[{"role": "user", "content": exercises_prompt}]
                )
                organized_parts.append(exercises_response.choices[0].message.content)
            except Exception as e:
                self.logger.warning(f"ç»ƒä¹ å†…å®¹ä¼˜åŒ–å¤±è´¥ï¼Œä½¿ç”¨åŸå†…å®¹: {str(e)}")
                organized_parts.append(content["exercises"])
        
        # åˆå¹¶æ‰€æœ‰å†…å®¹
        return "\n\n".join(organized_parts)

    def _parse_content(self, content: str) -> List[Dict[str, str]]:
        """è§£æ LLM è¿”å›çš„å†…å®¹ï¼Œå°†å…¶åˆ†å‰²æˆä¸åŒçš„éƒ¨åˆ†"""
        sections = []
        current_section = {"cell_type": "markdown", "source": []}
        lines = content.split("\n")
        i = 0
        
        def save_current_markdown():
            nonlocal current_section
            if current_section["source"]:
                sections.append({
                    "cell_type": "markdown",
                    "metadata": {},
                    "source": current_section["source"]
                })
                current_section = {"cell_type": "markdown", "source": []}
        
        while i < len(lines):
            line = lines[i].rstrip()
            
            # æ£€æŸ¥æ˜¯å¦æ˜¯ Python ä»£ç å—çš„å¼€å§‹
            if line.strip() == "```python":
                # ä¿å­˜å½“å‰çš„ markdown å†…å®¹
                save_current_markdown()
                
                # æ”¶é›†ä»£ç å†…å®¹
                code_lines = []
                i += 1  # è·³è¿‡å¼€å§‹æ ‡è®°
                while i < len(lines) and not lines[i].strip() == "```":
                    code_lines.append(lines[i].rstrip())
                    i += 1
                
                # åˆ›å»ºä»£ç å•å…ƒæ ¼
                if code_lines:
                    sections.append({
                        "cell_type": "code",
                        "metadata": {},
                        "source": code_lines,
                        "execution_count": None,
                        "outputs": []
                    })
            else:
                # å¤„ç†æ™®é€šæ–‡æœ¬
                current_section["source"].append(line)
            i += 1
        
        # ä¿å­˜æœ€åçš„ markdown å†…å®¹
        save_current_markdown()
        
        return sections

    def _convert_to_ipynb(self, content: str) -> str:
        """å°†å†…å®¹è½¬æ¢ä¸º ipynb æ ¼å¼"""
        try:
            # è§£æå†…å®¹
            sections = self._parse_content(content)
            
            # åˆ›å»ºæ–°çš„ notebook
            notebook = self.notebook_template.copy()
            
            # æ·»åŠ æ ‡é¢˜
            notebook["cells"].append({
                "cell_type": "markdown",
                "metadata": {},
                "source": ["# æœºå™¨å­¦ä¹ å®è®­è¯¾ç¨‹\n\n"]
            })
            
            # æ·»åŠ å„ä¸ªéƒ¨åˆ†çš„å†…å®¹
            for section in sections:
                cell = {
                    "cell_type": section["cell_type"],
                    "metadata": {},
                }
                
                if section["cell_type"] == "markdown":
                    # Markdown å•å…ƒæ ¼ä½¿ç”¨å•ä¸ªå­—ç¬¦ä¸²
                    cell["source"] = ["\n".join(section["source"])]
                else:
                    # ä»£ç å•å…ƒæ ¼ä¿æŒæ¯è¡Œç‹¬ç«‹
                    cell["source"] = [line + "\n" for line in section["source"]]
                    # æœ€åä¸€è¡Œä¸éœ€è¦é¢å¤–çš„æ¢è¡Œç¬¦
                    if cell["source"]:
                        cell["source"][-1] = cell["source"][-1].rstrip("\n")
                    cell["execution_count"] = None
                    cell["outputs"] = []
                
                notebook["cells"].append(cell)
            
            # è½¬æ¢ä¸º JSON å­—ç¬¦ä¸²
            return json.dumps(notebook, ensure_ascii=False, indent=2)
            
        except Exception as e:
            raise ValueError(f"è½¬æ¢ notebook æ ¼å¼æ—¶å‡ºé”™ï¼š{str(e)}")

    def _validate_notebook(self, notebook: str) -> None:
        """éªŒè¯ç”Ÿæˆçš„ notebook æ˜¯å¦æœ‰æ•ˆ"""
        try:
            notebook_dict = json.loads(notebook)
            required_fields = ['cells', 'metadata', 'nbformat', 'nbformat_minor']
            validate_llm_response(notebook_dict, required_fields)
            
            # éªŒè¯å•å…ƒæ ¼
            if not notebook_dict['cells']:
                raise ValidationError("Notebook ä¸èƒ½æ²¡æœ‰å†…å®¹")
            
            # éªŒè¯æ¯ä¸ªå•å…ƒæ ¼çš„æ ¼å¼
            for cell in notebook_dict['cells']:
                if 'cell_type' not in cell or 'source' not in cell:
                    raise ValidationError("å•å…ƒæ ¼æ ¼å¼æ— æ•ˆ")
                
        except json.JSONDecodeError:
            raise ValidationError("ç”Ÿæˆçš„ notebook ä¸æ˜¯æœ‰æ•ˆçš„ JSON æ ¼å¼")

    def _show_structure_preview(self, content: str) -> None:
        """æ˜¾ç¤º notebook ç»“æ„é¢„è§ˆ"""
        print("\nğŸ“” Notebook ç»“æ„é¢„è§ˆ:")
        print("-" * 40)
        
        # æå–æ‰€æœ‰æ ‡é¢˜
        sections = []
        current_level = 0
        
        for line in content.split("\n"):
            if line.startswith("#"):
                # è®¡ç®—æ ‡é¢˜çº§åˆ«
                level = len(line.split()[0])
                title = line.strip("#").strip()
                sections.append("  " * (level - 1) + "- " + title)
        
        # æ˜¾ç¤ºç›®å½•ç»“æ„
        print("ç›®å½•ç»“æ„:")
        for section in sections:
            print(section)
        
        # ç»Ÿè®¡ä¿¡æ¯
        code_blocks = content.count("```python")
        print(f"\nåŒ…å« {code_blocks} ä¸ªä»£ç å—")
        
        print("-" * 40)

    @RetryHandler().retry_on_exception
    def create_notebook(self, content: Dict[str, Any]) -> str:
        """å°†æ‰€æœ‰å†…å®¹ç»„è£…æˆ Jupyter notebook"""
        try:
            organized_content = self._organize_content(content)
            
            # åœ¨è½¬æ¢ä¹‹å‰æ˜¾ç¤ºç»“æ„é¢„è§ˆ
            self._show_structure_preview(organized_content)
            
            notebook = self._convert_to_ipynb(organized_content)
            self._validate_notebook(notebook)
            return notebook
            
        except Exception as e:
            self.logger.error(f"Notebook åˆ›å»ºå¤±è´¥: {str(e)}")
            raise ValueError(f"Notebook åˆ›å»ºå¤±è´¥: {str(e)}") 